{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing softened (well-differentiable) alternatives to `np.min`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use `npa.min` directly during optimisation, or do we need softened versions? The only way to know for sure is to test the autograd gradients of the optimisation FoM using both `npa.min` and the softened version. The softened version will guarantee consistency between the autograd gradient and finite differences, but the `npa.min` version may also be consistent, in which case the softened version is not needed. \n",
    "\n",
    "This problem is different from the issue with building the unit cell grid permittivities (`build_grating_gradable()` in `twobox.py`). There, each grid permittivity is set depending on an `if` condition for where the grid lies (inside box1, inside box2, between boxes, etc.). The `if` conditions themselves are not differentiable (they create non-continuous boundaries between `if` regions), hence `softmax` is necessary for FoM differentiability.\n",
    "\n",
    "Softmin and LogSumExp (LSE) can both be made numerically stable, and are both soft approximations to the min function. Which one to use depends on the speed and accuracy of each method, though they are similar enough that it probably doesn't matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1\n",
      "-0.8966647010107589\n",
      "-1.8148433677095008\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import autograd.numpy as npa\n",
    "from autograd import grad\n",
    "\n",
    "def softmin(sigma,p):\n",
    "    e_x = npa.exp(sigma*(npa.min(p) - p))\n",
    "    return e_x/npa.sum(e_x)\n",
    "\n",
    "def softmin_unstable(sigma,p):\n",
    "    # Unstable for very small negative values and/or sigma\n",
    "    e_x = npa.exp(-sigma*p)\n",
    "    return e_x/npa.sum(e_x)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return npa.min(x)\n",
    "\n",
    "def gradable_min_softmin(x,sigma=1):\n",
    "    \"\"\"Approximate min using expected value of x with probability distribution given by softmin\"\"\"\n",
    "    return npa.sum(x*softmin(sigma,x))\n",
    "\n",
    "def gradable_min_LSE(x,sigma=1):\n",
    "    \"\"\"Approximate min using LogSumExp\"\"\"\n",
    "    # From https://mathoverflow.net/questions/35191/a-differentiable-approximation-to-the-minimum-function    \n",
    "    ex = npa.exp(sigma * (npa.min(x) - x))\n",
    "    sumexp = npa.sum(ex)\n",
    "    logsumexp = npa.log(sumexp) - sigma*npa.min(x)\n",
    "    return -1/sigma * logsumexp\n",
    "\n",
    "\n",
    "x = npa.array([1.,-1.1,3.,-1.])\n",
    "print(f(x))\n",
    "print(gradable_min_softmin(x,1))\n",
    "print(gradable_min_LSE(x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1561b0bc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "sigma = 1.\n",
    "\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "XY = np.dstack((X,Y))\n",
    "\n",
    "standard_mins = np.minimum(X,Y)\n",
    "\n",
    "softmin_mins = np.zeros(X.shape)\n",
    "LSE_mins = np.zeros(X.shape)\n",
    "for i in range(XY.shape[0]):\n",
    "    for j in range(XY.shape[1]):\n",
    "        softmin_mins[i,j] = gradable_min_softmin(XY[i,j,:],sigma)\n",
    "        LSE_mins[i,j] = gradable_min_LSE(XY[i,j,:],sigma)\n",
    "\n",
    "fig, axs = plt.subplots(3,1, subplot_kw={\"projection\": \"3d\"}, figsize=(7,15))\n",
    "surf = axs[0].plot_surface(X, Y, standard_mins, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "surf1 = axs[1].plot_surface(X, Y, softmin_mins, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "surf2 = axs[2].plot_surface(X, Y, softmin_mins, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "for ax in axs:\n",
    "    ax.set_zlim(-5, 5)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "fig.colorbar(surf1, shrink=0.5, aspect=5)\n",
    "fig.colorbar(surf2, shrink=0.5, aspect=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n",
      "[-0.05372286  0.58875435 -0.02348758  0.4884561 ]\n",
      "[0.0599141  0.48926874 0.00810849 0.44270866]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "f_grad = grad(f)\n",
    "min_softmin_grad = grad(gradable_min_softmin)\n",
    "min_LSE_grad = grad(gradable_min_LSE)\n",
    "\n",
    "print(f_grad(x))\n",
    "print(min_softmin_grad(x))\n",
    "print(min_LSE_grad(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
