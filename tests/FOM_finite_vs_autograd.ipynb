{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOM finite difference vs integral of autograd derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can take the gradient of the figure of merit in two ways:\n",
    "* Finite difference using the FOM function directly\n",
    "* Use `adaptive` to integrate the autograd gradient of $F_D$\n",
    "\n",
    "Generally speaking, you use the first method as a baseline to check that the second method works correctly and go on to use the second method in your computations. But in this case, both calculations rely on `adaptive` integration over a finite number of sample points. This means that one method may converge faster than the other with respect to the number of sample points, so we don't know which one is the \"correct\" one. Generally want to use the second method, but not if it requires many more adaptive points to converge than the first method. Hence, to check the true answer, can use a very large number of sample points for both methods, ensure they both converge to the same somewhat-true value (still limited by numerical precision) and then compare the convergence rate of the two methods with respect to number of sample points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing autograd derivatives of FOM are consistent with finite difference derivatives\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from parameters import D1_ND\n",
    "import Optimisation.opt as opt\n",
    "from twobox import TwoBox\n",
    "\n",
    "\n",
    "# SETTINGS ################################################################################################\n",
    "np.random.seed(2025)\n",
    "abs_tol = 0.5\n",
    "rel_tol = 0.1\n",
    "Delta = 1e-7\n",
    "\n",
    "Nx = 100\n",
    "nG = 25\n",
    "Doppler = D1_ND([0.2,0])\n",
    "\n",
    "\n",
    "# Test gratings\n",
    "standard_params = [1.4, 0.7, 0.15, 0.35, 0.6, 9., 9., 20., 1., 1.45**2]\n",
    "standard_grating = TwoBox(*standard_params, wavelength=1., angle=0., Nx=Nx, nG=nG, Qabs=np.inf)\n",
    "# standard_grating.show_permittivity(show_analytic_box=True)\n",
    "\n",
    "near_cutoff_params = [1.999, 0.7, 0.15, 0.35, 0.6, 9., 9., 20., 1., 1.45**2] \n",
    "near_cutoff_grating = TwoBox(*near_cutoff_params, wavelength=1., angle=0., Nx=Nx, nG=nG, Qabs=np.inf)\n",
    "\n",
    "thin_pillars_params = [1.4, 0.7, 0.01, 0.01, 0.6, 9., 9., 20., 1., 1.45**2]\n",
    "thin_pillars_grating = TwoBox(*thin_pillars_params, wavelength=1., angle=0., Nx=Nx, nG=nG, Qabs=np.inf)\n",
    "\n",
    "closebox_params = [1.4, 0.7, 0.15, 0.35, 0.27, 9., 9., 20., 1., 1.45**2]\n",
    "closebox_grating = TwoBox(*closebox_params, wavelength=1., angle=0., Nx=Nx, nG=nG, Qabs=np.inf)\n",
    "\n",
    "overlap_params = [1.4, 0.7, 0.15, 0.35, 0.15, 12., 9., 20., 1., 1.45**2]\n",
    "overlap_grating = TwoBox(*overlap_params, wavelength=1., angle=0., Nx=Nx, nG=nG, Qabs=np.inf)\n",
    "\n",
    "clipbox_params = [1.4, 0.7, 0.35, 0.9, 1.1, 9., 9., 20., 1., 1.45**2]\n",
    "clipbox_grating = TwoBox(*clipbox_params, wavelength=1., angle=0., Nx=Nx, nG=nG, Qabs=np.inf)\n",
    "\n",
    "\n",
    "def FOM_params_func(grating, params, goal=0.5, return_grad=False):\n",
    "    grating_pitch, grating_depth, box1_width, box2_width, box_centre_dist, box1_eps, box2_eps, gaussian_width, substrate_depth, substrate_eps = params\n",
    "    grating.grating_pitch = grating_pitch\n",
    "    grating.grating_depth = grating_depth\n",
    "    grating.box1_width = box1_width\n",
    "    grating.box2_width = box2_width\n",
    "    grating.box_centre_dist = box_centre_dist\n",
    "    grating.box1_eps = box1_eps\n",
    "    grating.box2_eps = box2_eps\n",
    "    grating.gaussian_width = gaussian_width\n",
    "    grating.substrate_depth = substrate_depth\n",
    "    grating.substrate_eps = substrate_eps\n",
    "    return opt.FOM_uniform(grating, 20., goal, return_grad)\n",
    "\n",
    "\n",
    "def finite_grad(func, grating, params, Delta, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate gradient of 'func' at input 'params' using finite difference with respect to each component\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func    :   Function of grating\n",
    "    grating :   Grating to calculate finite difference\n",
    "    params  :   Parameters for func to differentiate with respect to\n",
    "    Delta   :   Finite difference step size for each component\n",
    "    kwargs  :   Kwargs to pass to func\n",
    "    \"\"\"\n",
    "\n",
    "    finitediff_coeffs = [-1/2, 1/2]\n",
    "    finite_diff_grad = np.zeros(len(params), dtype=np.float32)\n",
    "    for idx, p in enumerate(params):\n",
    "        params_plus = params[:]\n",
    "        params_minus = params[:]\n",
    "        \n",
    "        params_plus[idx] += Delta\n",
    "        params_minus[idx] -= Delta\n",
    "\n",
    "        dFOM_dp = 0\n",
    "        params_stepped = [params_minus, params_plus]\n",
    "        for c, pars in zip(finitediff_coeffs, params_stepped):\n",
    "            dFOM_dp += c*func(grating, pars, **kwargs)/Delta\n",
    "        finite_diff_grad[idx] = dFOM_dp\n",
    "    \n",
    "    return finite_diff_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate converged gradient (large number of sample points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrate autodiff: [-1.5105e+08 -1.9243e+08 -1.0984e+09  3.9706e+08  1.6090e+08 -1.8359e+07\n",
      "  9.9975e+06 -2.4341e+05  2.7280e+08  6.4256e+07]\n",
      "Raw FOM finitediff: [-1.5091e+08 -1.9244e+08 -1.0986e+09  3.9726e+08  1.6120e+08 -1.8356e+07\n",
      "  1.0218e+07 -2.4341e+05  2.7281e+08  6.4249e+07]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "# Choose test grating\n",
    "loss_goal = 0.001\n",
    "choose_grating = standard_grating\n",
    "grating_name = \"standard_grating\"\n",
    "\n",
    "test_grating = deepcopy(choose_grating)\n",
    "params = test_grating.params[:]\n",
    "\n",
    "# Finite diff\n",
    "finite_t0 = time.time()\n",
    "finite_diff_grad = finite_grad(FOM_params_func, test_grating, params, Delta, goal=loss_goal)\n",
    "finite_t1 = time.time()\n",
    "finite_runtime = finite_t1-finite_t0\n",
    "\n",
    "# Auto diff\n",
    "auto_t0 = time.time()\n",
    "auto_diff_grad = np.array(FOM_params_func(test_grating, params, goal=loss_goal, return_grad=True)[1])\n",
    "auto_t1 = time.time()\n",
    "auto_runtime = auto_t1-auto_t0\n",
    "\n",
    "# Save results\n",
    "fname = f\"./Data/{grating_name}_true_gradient_loss{loss_goal}_Nx{Nx}.pkl\"\n",
    "save_data = {'Grating': choose_grating, 'Loss goal': loss_goal, 'True finite grad': finite_diff_grad, 'True auto grad': auto_diff_grad}\n",
    "with open(fname, 'wb') as data_file:\n",
    "    pickle.dump(save_data, data_file)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "print(f\"Integrate autodiff: {auto_diff_grad}\")\n",
    "print(f\"Raw FOM finitediff: {finite_diff_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(fname, 'rb') as data_file:\n",
    "#     save_data = pickle.load(data_file)\n",
    "\n",
    "# auto_diff_grad = save_data[\"True auto grad\"]\n",
    "# finite_diff_grad = save_data[\"True finite grad\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate component-wise gradient error relative to converged gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose converged-gradient-grating parameters to retrieve\n",
    "Nx = 100\n",
    "loss_goal = 0.001\n",
    "choose_grating = standard_grating\n",
    "grating_name = \"standard_grating\"\n",
    "\n",
    "fname = f\"./Data/{grating_name}_true_gradient_loss{loss_goal}_Nx{Nx}.pkl\"\n",
    "with open(fname, 'rb') as data_file:\n",
    "    load_data = pickle.load(data_file)\n",
    "true_grad = load_data[\"True auto grad\"]\n",
    "\n",
    "# Test gradient convergence at different loss goals\n",
    "n_test = 10\n",
    "loss_goals = np.logspace(np.log10(0.005), -1, n_test, dtype=float)\n",
    "\n",
    "test_grating = deepcopy(choose_grating)\n",
    "params = test_grating.params[:]\n",
    "\n",
    "finite_diff_error = np.zeros((n_test,len(params)), dtype=np.float32)\n",
    "auto_diff_error = np.zeros((n_test,len(params)), dtype=np.float32)\n",
    "for g_idx, g in enumerate(loss_goals):\n",
    "    finite_diff_grad = finite_grad(FOM_params_func, test_grating, params, Delta, goal=g)\n",
    "    auto_diff_grad = np.array(FOM_params_func(test_grating, params, goal=g, return_grad=True)[1])\n",
    "\n",
    "    finite_diff_error[g_idx,:] = np.abs((finite_diff_grad - true_grad)/true_grad)\n",
    "    auto_diff_error[g_idx,:] = np.abs((auto_diff_grad - true_grad)/true_grad)\n",
    "\n",
    "fname = f\"./Data/{grating_name}_gradient_convergence_lossmin{loss_goals[0]:.2f}.pkl\"\n",
    "save_data = {'Grating': choose_grating, 'loss_goals': loss_goals, 'True gradient': true_grad,\n",
    "             'Finite grad error': finite_diff_error, 'Auto grad error': auto_diff_error}\n",
    "\n",
    "with open(fname, 'wb') as data_file:\n",
    "    pickle.dump(save_data, data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose converged-gradient-grating parameters to retrieve\n",
    "Nx = 100\n",
    "grating_name = \"standard_grating\"\n",
    "fname = f\"./Data/Nx{Nx}/{grating_name}_gradient_convergence_lossmin0.005.pkl\"\n",
    "with open(fname, 'rb') as data_file:\n",
    "    save_data = pickle.load(data_file)\n",
    "\n",
    "loss_goals = save_data[\"loss_goals\"]\n",
    "finite_diff_error = save_data[\"Finite grad error\"]\n",
    "auto_diff_error = save_data[\"Auto grad error\"]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8,7))\n",
    "\n",
    "shape = finite_diff_error.shape\n",
    "colors = ['#1F77B4', 'AEC7E8', '#FF7F0E', '#FFBB78', '#2CA02C', '#98DF8A', '#D62728', '#FF9896', '#9467BD', '#C5B0D5',\n",
    "          '#8C564B', '#C49C94', '#E377C2', '#F7B6D2', '#7F7F7F', '#C7C7C7', '#BCBD22', '#DBDB8D', '#17BECF', '#9EDAE5']  # From https://scottplot.net/cookbook/4.1/colors/#category-20\n",
    "labels = [\"Pitch\", \"Height\", \"Box 1 width\", \"Box 2 width\", \"Box centres separation\", \n",
    "          \"Box 1 epsilon\", \"Box 2 epsilon\", \"Gaussian width\", \"Substrate depth\", \"Substrate epsilon\"]\n",
    "for component in range(0,shape[1]):\n",
    "    axs[0].plot(loss_goals, 100*finite_diff_error[:,component], color=colors[component], linewidth=2, label=labels[component])\n",
    "    axs[1].plot(loss_goals, 100*auto_diff_error[:,component], color=colors[component], linewidth=2)\n",
    "\n",
    "axs[0].set(title=\"FOM finite difference\")\n",
    "axs[1].set(title=r\"Integrating autograd of $F_D$\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "fig.supxlabel(\"Loss goal\")\n",
    "fig.supylabel(\"Error relative to converged gradient (%)\")\n",
    "fig.legend(loc=\"center left\",bbox_to_anchor=(1.,0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"./Data/{grating_name}_gradient_convergence_lossmin{loss_goals[0]}.pdf\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
